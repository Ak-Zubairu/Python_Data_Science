#!/usr/bin/env python
# coding: utf-8

# ## Pandas is an API

# In[32]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# In[3]:


dict_={'a':[11,21,31],'b':[12,22,32]}

df=pd.DataFrame(dict_)
type(df)


# In[4]:


get_ipython().system('pip install nba_api')


# In[6]:


from nba_api.stats.static import teams
import matplotlib.pyplot as plt

def one_dict(list_dict):
    keys=list_dict[0].keys()
    out_dict={key:[] for key in keys}
    for dict_ in list_dict:
        for key, value in dict_.items():
            out_dict[key].append(value)
    return out_dict


# In[9]:


nba_teams = teams.get_teams()
nba_teams[0:2]


# In[10]:


dict_nba_team=one_dict(nba_teams)
df_teams=pd.DataFrame(dict_nba_team)
df_teams.head()


# In[11]:


df_warriors=df_teams[df_teams['nickname']=='Warriors']
df_warriors


# ### Practice Project: GDP Data extraction and processing
# 
# - Webscraping to extract required information from a website.
# - Pandas to load and process the tabular data as a dataframe.
# - Numpy to manipulate the information contatined in the dataframe.
# - Load the updated dataframe to CSV file.

# In[13]:


# Use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')


# In[16]:


URL="https://web.archive.org/web/20230902185326/https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29"

# Extract tables from webpage using Pandas. Retain table number 3 as the required dataframe.
tables = pd.read_html(URL)
df = tables[3]
df.head()


# In[23]:


df.shape


# In[18]:


# Replace the column headers with column numbers
df.columns = range(df.shape[1])
df.head()


# In[19]:


# Retain columns with index 0 and 2 (name of country and value of GDP quoted by IMF)
df = df[[0,2]]
df.head()


# In[27]:


# Retain the Rows with index 1 to 10, indicating the top 10 economies of the world.
df = df.iloc[1:11]
df


# In[21]:


# Assign column names as "Country" and "GDP (Million USD)"
df.columns = ['Country','GDP (Million USD)']
df


# In[28]:


# Change the data type of the 'GDP (Million USD)' column to integer. Use astype() method.
df['GDP (Million USD)'] = df['GDP (Million USD)'].astype(int)


# In[29]:


# Convert the GDP value in Million USD to Billion USD
df[['GDP (Million USD)']] = df[['GDP (Million USD)']]/1000


# In[33]:


# Use numpy.round() method to round the value to 2 decimal places.
df[['GDP (Million USD)']] = np.round(df[['GDP (Million USD)']], 2)


# In[34]:


# Rename the column header from 'GDP (Million USD)' to 'GDP (Billion USD)'
df.rename(columns = {'GDP (Million USD)' : 'GDP (Billion USD)'})


# In[35]:


# Load the DataFrame to the CSV file named "Largest_economies.csv"
df.to_csv('./Largest_economies.csv')


# ### Request in Python

# In[36]:


import requests

import os 
from PIL import Image
from IPython.display import IFrame


# In[37]:


url='https://www.ibm.com/'
r=requests.get(url)
r


# In[38]:


r.status_code


# In[41]:


header=r.headers
header['date']


# In[45]:


url='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/Example1.txt'
path=os.path.join(os.getcwd(),'example1.txt')
r=requests.get(url)
with open(path,'wb') as f:
    f.write(r.content)


# In[43]:


import requests
from bs4 import BeautifulSoup

# Specify the URL of the webpage you want to scrape
url = 'https://en.wikipedia.org/wiki/IBM'

# Send an HTTP GET request to the webpage
response = requests.get(url)

# Store the HTML content in a variable
html_content = response.text

# Create a BeautifulSoup object to parse the HTML
soup = BeautifulSoup(html_content, 'html.parser')

# Display a snippet of the HTML content
print(html_content[:500])


# ### RandomUser API 

# In[46]:


get_ipython().system('pip install randomuser')


# In[47]:


from randomuser import RandomUser


# In[49]:


r = RandomUser()
some_list = r.generate_users(3)
some_list


# In[55]:


for user in some_list:
    print (user.get_full_name()," ",user.get_email())


# In[58]:


for user in some_list:
    print (user.get_picture())


# In[59]:


def get_users():
    users =[]
     
    for user in RandomUser.generate_users(3):
        users.append({"Name":user.get_full_name(),"Gender":user.get_gender(),"City":user.get_city(),"State":user.get_state(),"Email":user.get_email(), "DOB":user.get_dob(),"Picture":user.get_picture()})

        
    return pd.DataFrame(users)     


# In[63]:


get_users()


# In[64]:


df1 = pd.DataFrame(get_users())
df1


# In[65]:


import json


# In[66]:


data = requests.get("https://fruityvice.com/api/fruit/all")


# In[67]:


results = json.loads(data.text)


# In[71]:


pd.DataFrame(results)


# In[72]:


df2 = pd.json_normalize(results)
df2


# ### Web Scraping with Python

# In[73]:


from bs4 import BeautifulSoup
import requests
URL = "http://www.example.com"
page = requests.get(URL)
soup = BeautifulSoup(page.content, "html.parser")


# In[74]:


import scrapy
class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = ['http://quotes.toscrape.com/tag/humor/',]
    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {'quote': quote.css('span.text::text').get()}


# In[ ]:




